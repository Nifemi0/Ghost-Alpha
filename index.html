<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Download kaggle_finetune.py</title>
    <style>
        body {
            font-family: sans-serif;
            margin: 2em;
        }
        pre {
            background-color: #f4f4f4;
            padding: 1em;
            border: 1px solid #ddd;
            border-radius: 5px;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        button {
            padding: 10px 20px;
            font-size: 1em;
            cursor: pointer;
            border: none;
            background-color: #007bff;
            color: white;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <h1>Updated kaggle_finetune.py</h1>
    <p>Click the button below to download the updated script.</p>
    <button onclick="download()">Download kaggle_finetune.py</button>
    <h2>Script Content:</h2>
    <pre><code id="code-content">
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import gc
import os
import joblib
from sqlalchemy import create_engine
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split

# --- Configuration ---
# Assumes you've uploaded your model, columns, and data to a Kaggle dataset named "poly-finetune-assets"
KAGGLE_INPUT_DIR = "/kaggle/input/poly-finetune-assets-automated" # Corrected path
KAGGLE_OUTPUT_DIR = "/kaggle/working/"

# --- Paths ---
# Path to the pre-trained model you uploaded
PRETRAINED_MODEL_PATH = os.path.join(KAGGLE_INPUT_DIR, "brain_v3.pt") 
# Path to the column definition file
COLUMN_PATH = os.path.join(KAGGLE_INPUT_DIR, "columns_v3.pkl")
# Path to your new dataset for fine-tuning
DB_PATH = os.path.join(KAGGLE_INPUT_DIR, "poly.db") # Assuming poly.db is in the dataset
# Path to save the new, fine-tuned model
FINETUNED_MODEL_PATH = os.path.join(KAGGLE_OUTPUT_DIR, "brain_v3_finetuned.pt")

# --- Device ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Model Definition ---
class NeuralPredictor(nn.Module):
    def __init__(self, input_dim):
        super(NeuralPredictor, self).__init__()
        self.layer_1 = nn.Linear(input_dim, 256)
        self.layer_2 = nn.Linear(256, 128)
        self.layer_3 = nn.Linear(128, 64)
        self.layer_out = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)
        self.batchnorm1 = nn.BatchNorm1d(256)
        self.batchnorm2 = nn.BatchNorm1d(128)
        self.batchnorm3 = nn.BatchNorm1d(64)

    def forward(self, x):
        x = self.layer_1(x)
        x = self.batchnorm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.layer_2(x)
        x = self.batchnorm2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.layer_3(x)
        x = self.batchnorm3(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.layer_out(x)
        return torch.sigmoid(x)

def fine_tune():
    # --- Feature Engineering ---
    if not os.path.exists(DB_PATH):
        print(f"Database not found at {DB_PATH}")
        return
        
    engine = create_engine(f"sqlite:///{DB_PATH}")
    print("Loading resolved events from database...")
    query = "SELECT * FROM events WHERE outcome IS NOT NULL"
    df = pd.read_sql(query, engine, parse_dates=['start_time', 'end_time'])

    if df.empty:
        print("No resolved events found.")
        return

    print(f"Engineering features for {len(df)} samples...")
    df['time_to_event_days'] = (df['end_time'] - df['start_time']).dt.days
    
    print("Generating text embeddings (Batch Mode)...")
    st_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    summaries = df['news_summary'].fillna('').tolist()
    all_embeddings = []
    batch_size = 1000
    
    for i in range(0, len(summaries), batch_size):
        batch = summaries[i:i + batch_size]
        batch_emb = st_model.encode(batch, show_progress_bar=False)
        all_embeddings.append(batch_emb)
        gc.collect()
    
    embeddings = np.vstack(all_embeddings)
    embedding_df = pd.DataFrame(embeddings, index=df.index, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])
    
    category_dummies = pd.get_dummies(df['category'], prefix='cat')
    numerical_features = df[['initial_price', 'volume', 'time_to_event_days']].fillna(0)
    
    features = pd.concat([numerical_features, category_dummies, embedding_df], axis=1)
    
    # Load the original columns to ensure consistency
    print(f"Loading column definitions from {COLUMN_PATH}")
    model_columns = joblib.load(COLUMN_PATH)
    
    # Align columns with the original model, filling missing ones with 0
    features = features.reindex(columns=model_columns, fill_value=0)
    
    # Ensure float32
    features = features.apply(pd.to_numeric, errors='coerce').fillna(0).astype('float32')
    
    target = df['outcome'].astype(int)

    # --- Initialize and Load Pre-trained Model ---
    input_dim = len(features.columns)
    model = NeuralPredictor(input_dim).to(device)

    print(f"Loading pre-trained weights from {PRETRAINED_MODEL_PATH}...")
    try:
        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))
        print("Model weights loaded successfully.")
    except Exception as e:
        print(f"Error loading model weights: {e}")
        print("Continuing with a newly initialized model.")

    # --- Data Splitting ---
    X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(
        features, target, test_size=0.2, random_state=42, stratify=target
    )
    
    X_train = torch.FloatTensor(X_train_pd.values).to(device)
    y_train = torch.FloatTensor(y_train_pd.values).view(-1, 1).to(device)
    
    del X_train_pd, y_train_pd, features, target # Clear RAM
    gc.collect()

    # --- Fine-tuning ---
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # Use a lower learning rate for fine-tuning
    criterion = nn.BCELoss()
    
    print("Starting Fine-Tuning...")
    for epoch in range(10): # Fewer epochs for fine-tuning
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        
        if (epoch+1) % 2 == 0: 
            print(f"Epoch {epoch+1} | Loss: {loss.item():.4f}")

    # --- Save the Fine-Tuned Model ---
    os.makedirs(KAGGLE_OUTPUT_DIR, exist_ok=True)
    torch.save(model.state_dict(), FINETUNED_MODEL_PATH)
    print(f"Fine-tuned model saved to {FINETUNED_MODEL_PATH}")

if __name__ == "__main__":
    fine_tune()

</code></pre>
    <script>
        function download() {
            var text = document.getElementById("code-content").textContent;
            var element = document.createElement('a');
            element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));
            element.setAttribute('download', 'kaggle_finetune.py');

            element.style.display = 'none';
            document.body.appendChild(element);

            element.click();

            document.body.removeChild(element);
        }
    </script>
</body>
</html>